package networking

import (
	"context"
	"fmt"
	"net"
	"os/exec"
	"path/filepath"
	"strconv"
	"strings"
	"time"

	g "github.com/onsi/ginkgo/v2"
	o "github.com/onsi/gomega"

	exutil "github.com/openshift/openshift-tests-private/test/extended/util"
	clusterinfra "github.com/openshift/openshift-tests-private/test/extended/util/clusterinfra"
	"k8s.io/apimachinery/pkg/util/wait"
	e2e "k8s.io/kubernetes/test/e2e/framework"
	e2enode "k8s.io/kubernetes/test/e2e/framework/node"
	e2eoutput "k8s.io/kubernetes/test/e2e/framework/pod/output"
)

var _ = g.Describe("[sig-networking] SDN service", func() {
	defer g.GinkgoRecover()

	var oc = exutil.NewCLI("networking-services", exutil.KubeConfigPath())
	// author: huirwang@redhat.com
	g.It("Author:huirwang-High-50347-[FdpOvnOvs] internalTrafficPolicy set Local for pod/node to service access", func() {
		var (
			buildPruningBaseDir    = exutil.FixturePath("testdata", "networking")
			pingPodNodeTemplate    = filepath.Join(buildPruningBaseDir, "ping-for-pod-specific-node-template.yaml")
			genericServiceTemplate = filepath.Join(buildPruningBaseDir, "service-generic-template.yaml")
		)

		nodeList, err := e2enode.GetReadySchedulableNodes(context.TODO(), oc.KubeFramework().ClientSet)
		o.Expect(err).NotTo(o.HaveOccurred())
		if len(nodeList.Items) < 2 {
			g.Skip("This case requires 2 nodes, but the cluster has less than two nodes")
		}
		g.By("Create a namespace")
		oc.SetupProject()
		ns1 := oc.Namespace()

		g.By("create 1st hello pod in ns1")

		pod1 := pingPodResourceNode{
			name:      "hello-pod1",
			namespace: ns1,
			nodename:  nodeList.Items[0].Name,
			template:  pingPodNodeTemplate,
		}
		pod1.createPingPodNode(oc)
		waitPodReady(oc, ns1, pod1.name)

		g.By("Create a test service which is in front of the above pods")
		svc := genericServiceResource{
			servicename:           "test-service",
			namespace:             ns1,
			protocol:              "TCP",
			selector:              "hello-pod",
			serviceType:           "ClusterIP",
			ipFamilyPolicy:        "",
			internalTrafficPolicy: "Local",
			externalTrafficPolicy: "", //This no value parameter will be ignored
			template:              genericServiceTemplate,
		}
		svc.ipFamilyPolicy = "SingleStack"
		svc.createServiceFromParams(oc)

		g.By("Create second namespace")
		oc.SetupProject()
		ns2 := oc.Namespace()

		g.By("Create a pod hello-pod2 in second namespace, pod located the same node")
		pod2 := pingPodResourceNode{
			name:      "hello-pod2",
			namespace: ns2,
			nodename:  nodeList.Items[0].Name,
			template:  pingPodNodeTemplate,
		}
		pod2.createPingPodNode(oc)
		waitPodReady(oc, ns2, pod2.name)

		g.By("Create second pod hello-pod3 in second namespace, pod located on the different node")
		pod3 := pingPodResourceNode{
			name:      "hello-pod3",
			namespace: ns2,
			nodename:  nodeList.Items[1].Name,
			template:  pingPodNodeTemplate,
		}
		pod3.createPingPodNode(oc)
		waitPodReady(oc, ns2, pod3.name)

		g.By("curl from hello-pod2 to service:port")
		CurlPod2SvcPass(oc, ns2, ns1, "hello-pod2", "test-service")

		g.By("curl from hello-pod3 to service:port should be failling")
		CurlPod2SvcFail(oc, ns2, ns1, "hello-pod3", "test-service")

		g.By("Curl from node0 to service:port")
		//Due to bug 2078691,skip below step for now.
		//CurlNode2SvcPass(oc, pod1.nodename, ns1,"test-service")
		g.By("Curl from node1 to service:port")
		CurlNode2SvcFail(oc, nodeList.Items[1].Name, ns1, "test-service")

		ipStackType := checkIPStackType(oc)

		if ipStackType == "dualstack" {
			g.By("Delete testservice from ns")
			err = oc.AsAdmin().WithoutNamespace().Run("delete").Args("svc", "test-service", "-n", ns1).Execute()
			o.Expect(err).NotTo(o.HaveOccurred())
			g.By("Checking pod to svc:port behavior now on with PreferDualStack Service")
			svc.ipFamilyPolicy = "PreferDualStack"
			svc.createServiceFromParams(oc)
			g.By("curl from hello-pod2 to service:port")
			CurlPod2SvcPass(oc, ns2, ns1, "hello-pod2", "test-service")

			g.By("curl from hello-pod3 to service:port should be failling")
			CurlPod2SvcFail(oc, ns2, ns1, "hello-pod3", "test-service")

			g.By("Curl from node0 to service:port")
			//Due to bug 2078691,skip below step for now.
			//CurlNode2SvcPass(oc, pod1.nodename, ns1,"test-service")
			g.By("Curl from node1 to service:port")
			CurlNode2SvcFail(oc, nodeList.Items[1].Name, ns1, "test-service")

		}
	})

	// author: huirwang@redhat.com
	g.It("Author:huirwang-High-50348-[FdpOvnOvs] internalTrafficPolicy set Local for pod/node to service access with hostnetwork pod backend. [Serial]", func() {
		var (
			buildPruningBaseDir    = exutil.FixturePath("testdata", "networking")
			hostNetworkPodTemplate = filepath.Join(buildPruningBaseDir, "ping-for-pod-hostnetwork-specific-node-template.yaml")
			pingPodNodeTemplate    = filepath.Join(buildPruningBaseDir, "ping-for-pod-specific-node-template.yaml")
			genericServiceTemplate = filepath.Join(buildPruningBaseDir, "service-generic-template.yaml")
		)

		nodeList, err := e2enode.GetReadySchedulableNodes(context.TODO(), oc.KubeFramework().ClientSet)
		o.Expect(err).NotTo(o.HaveOccurred())
		if len(nodeList.Items) < 2 {
			g.Skip("This case requires 2 nodes, but the cluster has less than two nodes")
		}
		g.By("Create a namespace")
		oc.SetupProject()
		ns1 := oc.Namespace()
		//Required for hostnetwork pod
		err = oc.AsAdmin().WithoutNamespace().Run("adm").Args("policy", "add-scc-to-group", "privileged", "system:serviceaccounts:"+ns1).Execute()
		o.Expect(err).NotTo(o.HaveOccurred())

		g.By("create 1st hello pod in ns1")

		pod1 := pingPodResourceNode{
			name:      "hello-pod1",
			namespace: ns1,
			nodename:  nodeList.Items[0].Name,
			template:  hostNetworkPodTemplate,
		}
		pod1.createPingPodNode(oc)
		waitPodReady(oc, ns1, pod1.name)

		g.By("Create a test service which is in front of the above pods")
		svc := genericServiceResource{
			servicename:           "test-service",
			namespace:             ns1,
			protocol:              "TCP",
			selector:              "hello-pod",
			serviceType:           "ClusterIP",
			ipFamilyPolicy:        "",
			internalTrafficPolicy: "Local",
			externalTrafficPolicy: "", //This no value parameter will be ignored
			template:              genericServiceTemplate,
		}
		svc.ipFamilyPolicy = "SingleStack"
		svc.createServiceFromParams(oc)

		g.By("Create second namespace")
		oc.SetupProject()
		ns2 := oc.Namespace()

		g.By("Create a pod hello-pod2 in second namespace, pod located the same node")
		pod2 := pingPodResourceNode{
			name:      "hello-pod2",
			namespace: ns2,
			nodename:  nodeList.Items[0].Name,
			template:  pingPodNodeTemplate,
		}
		pod2.createPingPodNode(oc)
		waitPodReady(oc, ns2, pod2.name)

		g.By("Create second pod hello-pod3 in second namespace, pod located on the different node")
		pod3 := pingPodResourceNode{
			name:      "hello-pod3",
			namespace: ns2,
			nodename:  nodeList.Items[1].Name,
			template:  pingPodNodeTemplate,
		}
		pod3.createPingPodNode(oc)
		waitPodReady(oc, ns2, pod3.name)

		g.By("curl from hello-pod2 to service:port")
		CurlPod2SvcPass(oc, ns2, ns1, "hello-pod2", "test-service")

		g.By("curl from hello-pod3 to service:port should be failing")
		CurlPod2SvcFail(oc, ns2, ns1, "hello-pod3", "test-service")

		g.By("Curl from node1 to service:port")
		CurlNode2SvcFail(oc, nodeList.Items[1].Name, ns1, "test-service")

		ipStackType := checkIPStackType(oc)

		if ipStackType == "dualstack" {
			g.By("Delete testservice from ns")
			err = oc.AsAdmin().WithoutNamespace().Run("delete").Args("svc", "test-service", "-n", ns1).Execute()
			o.Expect(err).NotTo(o.HaveOccurred())
			g.By("Checking pod to svc:port behavior now on with PreferDualStack Service")
			svc.ipFamilyPolicy = "PreferDualStack"
			svc.createServiceFromParams(oc)
			g.By("curl from hello-pod2 to service:port")
			CurlPod2SvcPass(oc, ns2, ns1, "hello-pod2", "test-service")

			g.By("curl from hello-pod3 to service:port should be failing")
			CurlPod2SvcFail(oc, ns2, ns1, "hello-pod3", "test-service")

			g.By("Curl from node1 to service:port")
			CurlNode2SvcFail(oc, nodeList.Items[1].Name, ns1, "test-service")

		}
	})

	// author: weliang@redhat.com
	g.It("Author:weliang-Medium-57344-Add support for service session affinity timeout", func() {
		//Bug: https://issues.redhat.com/browse/OCPBUGS-4502
		var (
			buildPruningBaseDir         = exutil.FixturePath("testdata", "networking")
			servicesBaseDir             = exutil.FixturePath("testdata", "networking/services")
			pingPodTemplate             = filepath.Join(buildPruningBaseDir, "ping-for-pod-template.yaml")
			sessionAffinitySvcv4        = filepath.Join(servicesBaseDir, "sessionaffinity-svcv4.yaml")
			sessionAffinitySvcdualstack = filepath.Join(servicesBaseDir, "sessionaffinity-svcdualstack.yaml")
			sessionAffinityPod1         = filepath.Join(servicesBaseDir, "sessionaffinity-pod1.yaml")
			sessionAffinityPod2         = filepath.Join(servicesBaseDir, "sessionaffinity-pod2.yaml")
		)

		ns1 := oc.Namespace()

		g.By("create two pods which will be the endpoints for sessionaffinity service in ns1")
		defer oc.AsAdmin().WithoutNamespace().Run("delete").Args("-f", sessionAffinityPod1, "-n", ns1).Execute()
		defer oc.AsAdmin().WithoutNamespace().Run("delete").Args("-f", sessionAffinityPod2, "-n", ns1).Execute()
		createResourceFromFile(oc, ns1, sessionAffinityPod1)
		waitPodReady(oc, ns1, "blue-pod-1")
		createResourceFromFile(oc, ns1, sessionAffinityPod2)
		waitPodReady(oc, ns1, "blue-pod-2")

		g.By("create a testing pod in ns1")
		pod1 := pingPodResource{
			name:      "hello-pod1",
			namespace: ns1,
			template:  pingPodTemplate,
		}
		defer oc.AsAdmin().WithoutNamespace().Run("delete").Args("pod", pod1.name, "-n", pod1.namespace).Execute()
		pod1.createPingPod(oc)
		waitPodReady(oc, ns1, pod1.name)

		ipStackType := checkIPStackType(oc)
		if ipStackType == "ipv4single" {
			g.By("test ipv4 singlestack cluster")
			g.By("create a sessionaffinity service in ns1")
			defer oc.AsAdmin().WithoutNamespace().Run("delete").Args("-f", sessionAffinitySvcv4, "-n", ns1).Execute()
			createsvcerr := oc.AsAdmin().WithoutNamespace().Run("create").Args("-f", sessionAffinitySvcv4, "-n", ns1).Execute()
			o.Expect(createsvcerr).NotTo(o.HaveOccurred())
			svcoutput, svcerr := oc.AsAdmin().Run("get").Args("service", "-n", ns1).Output()
			o.Expect(svcerr).NotTo(o.HaveOccurred())
			o.Expect(svcoutput).To(o.ContainSubstring("sessionaffinitysvcv4"))
			serviceIPv4 := getSvcIPv4(oc, ns1, "sessionaffinitysvcv4")

			// timeoutSeconds in sessionAffinityConfig is set 10s, traffic will LB after curl sleep more than 10s
			g.By("Traffic will LB to two endpoints with sleep 15s in curl")
			trafficoutput, trafficerr := e2eoutput.RunHostCmd(ns1, pod1.name, "for i in 1 2 3 4 5 6 7 8 9 10; do curl "+serviceIPv4+":8080; sleep 11; done")
			o.Expect(trafficerr).NotTo(o.HaveOccurred())
			if strings.Contains(trafficoutput, "Hello Blue Pod-1") && strings.Contains(trafficoutput, "Hello Blue Pod-2") {
				e2e.Logf("Pass : Traffic LB to two endpoints when curl sleep more than 10s")
			} else {
				e2e.Failf("Fail: Traffic does not LB to two endpoints when curl sleep more than 10s")
			}

			// timeoutSeconds in sessionAffinityConfig is set 10s, traffic will not LB after curl sleep less than 10s
			g.By("Traffic will not LB to two endpoints without sleep 15s in curl")
			trafficoutput1, trafficerr1 := e2eoutput.RunHostCmd(ns1, pod1.name, "for i in 1 2 3 4 5 6 7 8 9 10; do curl "+serviceIPv4+":8080; sleep 9; done")
			o.Expect(trafficerr1).NotTo(o.HaveOccurred())
			if (strings.Contains(trafficoutput1, "Hello Blue Pod-1") && !strings.Contains(trafficoutput1, "Hello Blue Pod-2")) || (strings.Contains(trafficoutput1, "Hello Blue Pod-2") && !strings.Contains(trafficoutput1, "Hello Blue Pod-1")) {
				e2e.Logf("Pass : Traffic does not LB to two endpoints when curl sleep less than 10s")
			} else {
				e2e.Failf("Fail: Traffic LB to two endpoints when curl sleep less than 10s")
			}
		}

		if ipStackType == "dualstack" {
			g.By("test dualstack cluster")
			g.By("create a sessionaffinity service in ns1")
			defer oc.AsAdmin().WithoutNamespace().Run("delete").Args("-f", sessionAffinitySvcdualstack, "-n", ns1).Execute()
			createsvcerr := oc.AsAdmin().WithoutNamespace().Run("create").Args("-f", sessionAffinitySvcdualstack, "-n", ns1).Execute()
			o.Expect(createsvcerr).NotTo(o.HaveOccurred())
			svcoutput, svcerr := oc.AsAdmin().Run("get").Args("service", "-n", ns1).Output()
			o.Expect(svcerr).NotTo(o.HaveOccurred())
			o.Expect(svcoutput).To(o.ContainSubstring("sessionaffinitysvcdualstack"))
			serviceIPv4 := getSvcIPv4(oc, ns1, "sessionaffinitysvcdualstack")
			serviceIPv6 := getSvcIPv6(oc, ns1, "sessionaffinitysvcdualstack")

			// Test ipv4 traffic in dualstack cluster
			// timeoutSeconds in sessionAffinityConfig is set 10s, traffic will LB after curl sleep more than 10s
			g.By("Traffic will LB to two endpoints with sleep 15s in curl")
			trafficoutput, trafficerr := e2eoutput.RunHostCmd(ns1, pod1.name, "for i in 1 2 3 4 5 6 7 8 9 10; do curl "+serviceIPv4+":8080; sleep 11; done")
			o.Expect(trafficerr).NotTo(o.HaveOccurred())
			if strings.Contains(trafficoutput, "Hello Blue Pod-1") && strings.Contains(trafficoutput, "Hello Blue Pod-2") {
				e2e.Logf("Pass : Traffic LB to two endpoints when curl sleep more than 10s")
			} else {
				e2e.Failf("Fail: Traffic does not LB to two endpoints when curl sleep more than 10s")
			}

			// timeoutSeconds in sessionAffinityConfig is set 10s, traffic will not LB after curl sleep less than 10s
			g.By("Traffic will not LB to two endpoints without sleep 15s in curl")
			trafficoutput1, trafficerr1 := e2eoutput.RunHostCmd(ns1, pod1.name, "for i in 1 2 3 4 5 6 7 8 9 10; do curl "+serviceIPv4+":8080; sleep 9; done")
			o.Expect(trafficerr1).NotTo(o.HaveOccurred())
			if (strings.Contains(trafficoutput1, "Hello Blue Pod-1") && !strings.Contains(trafficoutput1, "Hello Blue Pod-2")) || (strings.Contains(trafficoutput1, "Hello Blue Pod-2") && !strings.Contains(trafficoutput1, "Hello Blue Pod-1")) {
				e2e.Logf("Pass : Traffic does not LB to two endpoints when curl sleep less than 10s")
			} else {
				e2e.Failf("Fail: Traffic LB to two endpoints when curl sleep less than 10s")
			}

			// Tes ipv6 traffic in dualstack cluster
			// timeoutSeconds in sessionAffinityConfig is set 10s, traffic will LB after curl sleep more than 10s
			g.By("Traffic will LB to two endpoints with sleep 15s in curl")
			v6trafficoutput, v6trafficerr := e2eoutput.RunHostCmd(ns1, pod1.name, "for i in 1 2 3 4 5 6 7 8 9 10; do curl -g -6 ["+serviceIPv6+"]:8080; sleep 11; done")
			o.Expect(v6trafficerr).NotTo(o.HaveOccurred())
			if strings.Contains(v6trafficoutput, "Hello Blue Pod-1") && strings.Contains(v6trafficoutput, "Hello Blue Pod-2") {
				e2e.Logf("Pass : Traffic LB to two endpoints when curl sleep more than 10s")
			} else {
				e2e.Failf("Fail: Traffic does not LB to two endpoints when curl sleep more than 10s")
			}

			// timeoutSeconds in sessionAffinityConfig is set 10s, traffic will not LB after curl sleep less than 10s
			g.By("Traffic will not LB to two endpoints without sleep 15s in curl")
			v6trafficoutput1, v6trafficerr1 := e2eoutput.RunHostCmd(ns1, pod1.name, "for i in 1 2 3 4 5 6 7 8 9 10; do curl -g -6 ["+serviceIPv6+"]:8080; sleep 9; done")
			o.Expect(v6trafficerr1).NotTo(o.HaveOccurred())
			if (strings.Contains(v6trafficoutput1, "Hello Blue Pod-1") && !strings.Contains(v6trafficoutput1, "Hello Blue Pod-2")) || (strings.Contains(v6trafficoutput1, "Hello Blue Pod-2") && !strings.Contains(v6trafficoutput1, "Hello Blue Pod-1")) {
				e2e.Logf("Pass : Traffic does not LB to two endpoints when curl sleep less than 10s")
			} else {
				e2e.Failf("Fail: Traffic LB to two endpoints when curl sleep less than 10s")
			}
		}
	})
	// author: asood@redhat.com
	g.It("Longduration-NonPreRelease-Author:asood-High-62293-[FdpOvnOvs] Validate all the constructs are created on logical routers and logical switches for a service type loadbalancer. [Disruptive]", func() {
		// Bug: https://issues.redhat.com/browse/OCPBUGS-5930 (Duplicate bug https://issues.redhat.com/browse/OCPBUGS-7000)
		var (
			buildPruningBaseDir    = exutil.FixturePath("testdata", "networking")
			pingPodNodeTemplate    = filepath.Join(buildPruningBaseDir, "ping-for-pod-specific-node-template.yaml")
			genericServiceTemplate = filepath.Join(buildPruningBaseDir, "service-generic-template.yaml")
			svcEndpoints           []svcEndpontDetails
			lsConstruct            string
			lrConstruct            string
		)
		platform := exutil.CheckPlatform(oc)
		//vSphere does not have LB service support yet
		e2e.Logf("platform %s", platform)
		if !(strings.Contains(platform, "gcp") || strings.Contains(platform, "aws") || strings.Contains(platform, "azure")) {
			g.Skip("Skip for non-supported auto scaling machineset platforms!!")
		}
		networkType := exutil.CheckNetworkType(oc)
		o.Expect(networkType).NotTo(o.BeEmpty())
		if networkType != "ovnkubernetes" {
			g.Skip("OVN constructs would not be on the cluster")
		}
		workerNodes, err := exutil.GetClusterNodesBy(oc, "worker")
		o.Expect(err).NotTo(o.HaveOccurred())

		exutil.By("Get namespace")
		ns := oc.Namespace()

		exutil.By(fmt.Sprintf("create 1st hello pod in %s", ns))
		pod := pingPodResourceNode{
			name:      "hello-pod1",
			namespace: ns,
			nodename:  workerNodes[0],
			template:  pingPodNodeTemplate,
		}
		pod.createPingPodNode(oc)
		waitPodReady(oc, ns, pod.name)

		exutil.By("Create a test service which is in front of the above pod")
		svc := genericServiceResource{
			servicename:           "test-service",
			namespace:             ns,
			protocol:              "TCP",
			selector:              "hello-pod",
			serviceType:           "LoadBalancer",
			ipFamilyPolicy:        "SingleStack",
			internalTrafficPolicy: "Cluster",
			externalTrafficPolicy: "Cluster",
			template:              genericServiceTemplate,
		}
		svc.createServiceFromParams(oc)
		exutil.By("Create a new machineset to add new nodes")
		clusterinfra.SkipConditionally(oc)
		infrastructureName := clusterinfra.GetInfrastructureName(oc)
		machinesetName := infrastructureName + "-62293"
		ms := clusterinfra.MachineSetDescription{Name: machinesetName, Replicas: 2}
		defer ms.DeleteMachineSet(oc)
		ms.CreateMachineSet(oc)
		clusterinfra.WaitForMachinesRunning(oc, 2, machinesetName)
		machineName := clusterinfra.GetMachineNamesFromMachineSet(oc, machinesetName)
		nodeName0 := clusterinfra.GetNodeNameFromMachine(oc, machineName[0])
		nodeName1 := clusterinfra.GetNodeNameFromMachine(oc, machineName[1])
		e2e.Logf("The nodes %s and %s added successfully", nodeName0, nodeName1)

		exutil.By(fmt.Sprintf("create 2nd hello pod in %s on newly created node %s", ns, nodeName0))
		pod = pingPodResourceNode{
			name:      "hello-pod2",
			namespace: ns,
			nodename:  nodeName0,
			template:  pingPodNodeTemplate,
		}
		pod.createPingPodNode(oc)
		waitPodReady(oc, ns, pod.name)

		exutil.By("Get backend pod details of user service")
		allPods, getPodErr := exutil.GetAllPodsWithLabel(oc, ns, "name=hello-pod")
		o.Expect(getPodErr).NotTo(o.HaveOccurred())
		o.Expect(len(allPods)).NotTo(o.BeEquivalentTo(0))
		for _, eachPod := range allPods {
			nodeName, nodeNameErr := exutil.GetPodNodeName(oc, ns, eachPod)
			o.Expect(nodeNameErr).NotTo(o.HaveOccurred())
			podIP := getPodIPv4(oc, ns, eachPod)
			ovnkubeNodePod, ovnKubeNodePodErr := exutil.GetPodName(oc, "openshift-ovn-kubernetes", "app=ovnkube-node", nodeName)
			o.Expect(ovnKubeNodePodErr).NotTo(o.HaveOccurred())
			svcEndpoint := svcEndpontDetails{
				ovnKubeNodePod: ovnkubeNodePod,
				nodeName:       nodeName,
				podIP:          podIP,
			}
			svcEndpoints = append(svcEndpoints, svcEndpoint)
		}

		exutil.By("Get logical route and switch on node for endpoints of both services to validate they exist on both new and old node")
		for _, eachEndpoint := range svcEndpoints {
			lsConstruct = eachEndpoint.getOVNConstruct(oc, "ls-list")
			o.Expect(lsConstruct).NotTo(o.BeEmpty())
			e2e.Logf("Logical Switch %s on node %s", lsConstruct, eachEndpoint.nodeName)
			o.Expect(eachEndpoint.getOVNLBContruct(oc, "ls-lb-list", lsConstruct)).To(o.BeTrue())
			lrConstruct = eachEndpoint.getOVNConstruct(oc, "lr-list")
			o.Expect(lrConstruct).NotTo(o.BeEmpty())
			e2e.Logf("Logical Router %s on node %s", lrConstruct, eachEndpoint.nodeName)
			o.Expect(eachEndpoint.getOVNLBContruct(oc, "lr-lb-list", lrConstruct)).To(o.BeTrue())
		}

		exutil.By("Validate kubernetes service is reachable from all nodes including new nodes")
		allNodes, nodeErr := exutil.GetAllNodes(oc)
		o.Expect(nodeErr).NotTo(o.HaveOccurred())
		o.Expect(len(allNodes)).NotTo(o.BeEquivalentTo(0))
		for i := 0; i < len(allNodes); i++ {
			output, err := exutil.DebugNodeWithChroot(oc, allNodes[i], "bash", "-c", "curl -s -k https://172.30.0.1/healthz --connect-timeout 5")
			o.Expect(err).NotTo(o.HaveOccurred())
			o.Expect(strings.Contains(output, "ok")).To(o.BeTrue())
		}

	})
	// author: asood@redhat.com
	g.It("Longduration-NonPreRelease-Author:asood-High-63156-Verify the nodeport is not allocated to VIP based LoadBalancer service type. [Disruptive]", func() {
		// LoadBalancer service implementation are different on cloud provider and bare metal platform
		// https://issues.redhat.com/browse/OCPBUGS-10874 (aws and azure pending support)
		var (
			testDataDir                 = exutil.FixturePath("testdata", "networking/metallb")
			loadBalancerServiceTemplate = filepath.Join(testDataDir, "loadbalancer-svc-template.yaml")
			serviceLabelKey             = "environ"
			serviceLabelValue           = "Test"
			svc_names                   = [2]string{"hello-world-cluster", "hello-world-local"}
			svc_etp                     = [2]string{"Cluster", "Local"}
			namespaces                  []string
		)
		platform := exutil.CheckPlatform(oc)
		e2e.Logf("platform %s", platform)
		if !(strings.Contains(platform, "gcp")) {
			g.Skip("Skip for non-supported platorms!")
		}
		masterNodes, err := exutil.GetClusterNodesBy(oc, "master")
		o.Expect(err).NotTo(o.HaveOccurred())

		g.By("Get first namespace and create another")
		ns := oc.Namespace()
		namespaces = append(namespaces, ns)
		oc.SetupProject()
		ns = oc.Namespace()
		namespaces = append(namespaces, ns)
		var desiredMode string
		origMode := getOVNGatewayMode(oc)
		defer switchOVNGatewayMode(oc, origMode)
		g.By("Validate services in original gateway mode " + origMode)
		for j := 0; j < 2; j++ {
			for i := 0; i < 2; i++ {
				svcName := svc_names[i] + "-" + strconv.Itoa(j)
				g.By("Create a service " + svc_names[i] + " with ExternalTrafficPolicy " + svc_etp[i])
				svc := loadBalancerServiceResource{
					name:                          svcName,
					namespace:                     namespaces[i],
					externaltrafficpolicy:         svc_etp[i],
					labelKey:                      serviceLabelKey,
					labelValue:                    serviceLabelValue,
					allocateLoadBalancerNodePorts: false,
					template:                      loadBalancerServiceTemplate,
				}
				result := createLoadBalancerService(oc, svc, loadBalancerServiceTemplate)
				o.Expect(result).To(o.BeTrue())

				g.By("Check LoadBalancer service status")
				err = checkLoadBalancerSvcStatus(oc, svc.namespace, svc.name)
				o.Expect(err).NotTo(o.HaveOccurred())
				g.By("Get LoadBalancer service IP")
				svcIP := getLoadBalancerSvcIP(oc, svc.namespace, svc.name)
				g.By("Validate service")
				result = validateService(oc, masterNodes[0], svcIP)
				o.Expect(result).To(o.BeTrue())
				g.By("Check nodePort is not assigned to service")
				nodePort := getLoadBalancerSvcNodePort(oc, svc.namespace, svc.name)
				o.Expect(nodePort).To(o.BeEmpty())
			}
			if j == 0 {
				g.By("Change the shared gateway mode to local gateway mode")
				if origMode == "local" {
					desiredMode = "shared"
				} else {
					desiredMode = "local"
				}
				e2e.Logf("Cluster is currently on gateway mode %s", origMode)
				e2e.Logf("Desired mode is %s", desiredMode)

				switchOVNGatewayMode(oc, desiredMode)
				g.By("Validate services in modified gateway mode " + desiredMode)
			}
		}

	})

	// author: huirwang@redhat.com
	g.It("NonHyperShiftHOST-NonPreRelease-Longduration-Author:huirwang-Medium-65796-Recreated service should have correct load_balancer nb entries for same name load_balancer. [Serial]", func() {
		// From customer bug https://issues.redhat.com/browse/OCPBUGS-11716
		var (
			buildPruningBaseDir    = exutil.FixturePath("testdata", "networking")
			pingPodTemplate        = filepath.Join(buildPruningBaseDir, "ping-for-pod-template.yaml")
			genericServiceTemplate = filepath.Join(buildPruningBaseDir, "service-generic-template.yaml")
		)

		exutil.By("Get namespace ")
		ns := oc.Namespace()

		exutil.By("create hello pod in namespace")
		pod1 := pingPodResource{
			name:      "hello-pod",
			namespace: ns,
			template:  pingPodTemplate,
		}
		pod1.createPingPod(oc)
		waitPodReady(oc, ns, pod1.name)

		ipStack := checkIPStackType(oc)
		var podIPv6, podIPv4 string
		if ipStack == "dualstack" {
			podIPv6, podIPv4 = getPodIP(oc, ns, pod1.name)
		} else if ipStack == "ipv6single" {
			podIPv6, _ = getPodIP(oc, ns, pod1.name)
		} else {
			podIPv4, _ = getPodIP(oc, ns, pod1.name)
		}

		exutil.By("Create a test service which is in front of the above pods")
		svc := genericServiceResource{
			servicename:           "test-service",
			namespace:             ns,
			protocol:              "TCP",
			selector:              "hello-pod",
			serviceType:           "ClusterIP",
			ipFamilyPolicy:        "",
			internalTrafficPolicy: "Cluster",
			externalTrafficPolicy: "", //This no value parameter will be ignored
			template:              genericServiceTemplate,
		}

		if ipStack == "dualstack" {
			svc.ipFamilyPolicy = "PreferDualStack"
		} else {
			svc.ipFamilyPolicy = "SingleStack"
		}
		svc.createServiceFromParams(oc)

		exutil.By("Check service status")
		svcOutput, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("service", "-n", ns, svc.servicename).Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(svcOutput).Should(o.ContainSubstring(svc.servicename))

		exutil.By("Get service IP")
		var svcIP6, svcIP4, clusterVIP string
		if ipStack == "dualstack" || ipStack == "ipv6single" {
			svcIP6, svcIP4 = getSvcIP(oc, svc.namespace, svc.servicename)
		} else {
			svcIP4, _ = getSvcIP(oc, svc.namespace, svc.servicename)
		}
		e2e.Logf("ipstack type: %s, SVC's IPv4: %s, SVC's IPv6: %s", ipStack, svcIP4, svcIP6)

		exutil.By("Check nb loadbalancer entries")
		ovnPod := getOVNKMasterOVNkubeNode(oc)
		o.Expect(ovnPod).ShouldNot(o.BeEmpty())
		e2e.Logf("\n ovnKMasterPod: %v\n", ovnPod)
		lbCmd := fmt.Sprintf("ovn-nbctl --column vip find load_balancer name=Service_%s/%s_TCP_cluster", ns, svc.servicename)
		lbOutput, err := exutil.RemoteShPodWithBash(oc, "openshift-ovn-kubernetes", ovnPod, lbCmd)
		e2e.Logf("\nlbOutput: %s\n", lbOutput)
		o.Expect(err).NotTo(o.HaveOccurred())
		if ipStack == "dualstack" || ipStack == "ipv6single" {
			clusterVIP = fmt.Sprintf("\"[%s]:%s\"=\"[%s]:%s\"", svcIP6, "27017", podIPv6, "8080")
			o.Expect(lbOutput).Should(o.ContainSubstring(clusterVIP))

		}
		if ipStack == "dualstack" || ipStack == "ipv4single" {
			clusterVIP = fmt.Sprintf("\"%s:%s\"=\"%s:%s\"", svcIP4, "27017", podIPv4, "8080")
			o.Expect(lbOutput).Should(o.ContainSubstring(clusterVIP))
		}

		exutil.By("Delete svc")
		removeResource(oc, true, true, "service", svc.servicename, "-n", ns)

		exutil.By("Manually add load_balancer entry in nb with same name as previous one.")
		// no need to defer to remove, as this will be overrided by following service recreated.
		var lbCmdAdd string
		if ipStack == "dualstack" || ipStack == "ipv6single" {
			lbCmdAdd = fmt.Sprintf("ovn-nbctl lb-add \"Service_%s/%s_TCP_cluster\" [%s]:%s [%s]:%s", ns, svc.servicename, svcIP6, "27017", podIPv6, "8080")
			_, err = exutil.RemoteShPodWithBash(oc, "openshift-ovn-kubernetes", ovnPod, lbCmdAdd)
			o.Expect(err).NotTo(o.HaveOccurred())
		}
		if ipStack == "dualstack" || ipStack == "ipv4single" {
			lbCmdAdd = fmt.Sprintf("ovn-nbctl lb-add \"Service_%s/%s_TCP_cluster\" %s:%s %s:%s", ns, svc.servicename, svcIP4, "27017", podIPv4, "8080")
			_, err = exutil.RemoteShPodWithBash(oc, "openshift-ovn-kubernetes", ovnPod, lbCmdAdd)
			o.Expect(err).NotTo(o.HaveOccurred())
		}

		exutil.By("Recreate svc")
		svc.createServiceFromParams(oc)
		svcOutput, err = oc.AsAdmin().WithoutNamespace().Run("get").Args("service", "-n", ns, svc.servicename).Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(svcOutput).Should(o.ContainSubstring(svc.servicename))

		exutil.By("Get service IP again")
		if ipStack == "dualstack" || ipStack == "ipv6single" {
			svcIP6, svcIP4 = getSvcIP(oc, svc.namespace, svc.servicename)
		} else {
			svcIP4, _ = getSvcIP(oc, svc.namespace, svc.servicename)
		}
		e2e.Logf("ipstack type: %s, recreated SVC's IPv4: %s, SVC's IPv6: %s", ipStack, svcIP4, svcIP6)

		exutil.By("No error logs")
		podlogs, getLogsErr := oc.AsAdmin().Run("logs").Args(ovnPod, "-n", "openshift-ovn-kubernetes", "-c", "ovnkube-controller", "--since", "90s").Output()
		o.Expect(getLogsErr).NotTo(o.HaveOccurred())
		o.Expect(strings.Contains(podlogs, "failed to ensure service")).ShouldNot(o.BeTrue())

		exutil.By("Check nb load_balancer entries again!")
		lbOutput, err = exutil.RemoteShPodWithBash(oc, "openshift-ovn-kubernetes", ovnPod, lbCmd)
		e2e.Logf("\nlbOutput after SVC recreated: %s\n", lbOutput)
		o.Expect(err).NotTo(o.HaveOccurred())
		if ipStack == "dualstack" || ipStack == "ipv6single" {
			clusterVIP = fmt.Sprintf("\"[%s]:%s\"=\"[%s]:%s\"", svcIP6, "27017", podIPv6, "8080")
			o.Expect(lbOutput).Should(o.ContainSubstring(clusterVIP))

		}
		if ipStack == "dualstack" || ipStack == "ipv4single" {
			clusterVIP = fmt.Sprintf("\"%s:%s\"=\"%s:%s\"", svcIP4, "27017", podIPv4, "8080")
			o.Expect(lbOutput).Should(o.ContainSubstring(clusterVIP))
		}

		exutil.By("Validate service")
		CurlPod2SvcPass(oc, ns, ns, pod1.name, svc.servicename)
	})

	// author: asood@redhat.com
	g.It("Author:asood-High-46015-[FdpOvnOvs] Verify traffic to outside the cluster redirected when OVN is used and NodePort service is configured.", func() {
		// Customer bug https://bugzilla.redhat.com/show_bug.cgi?id=1946696
		var (
			buildPruningBaseDir    = exutil.FixturePath("testdata", "networking")
			pingPodNodeTemplate    = filepath.Join(buildPruningBaseDir, "ping-for-pod-specific-node-template.yaml")
			genericServiceTemplate = filepath.Join(buildPruningBaseDir, "service-generic-template.yaml")
		)
		exutil.By("0. Check the network plugin")
		networkType := exutil.CheckNetworkType(oc)
		o.Expect(networkType).NotTo(o.BeEmpty())
		if networkType != "ovnkubernetes" {
			g.Skip("Unsupported network plugin for this test")
		}
		ipStackType := checkIPStackType(oc)
		o.Expect(ipStackType).NotTo(o.BeEmpty())

		exutil.By("1. Get list of worker nodes")
		nodeList, err := e2enode.GetReadySchedulableNodes(context.TODO(), oc.KubeFramework().ClientSet)
		o.Expect(err).NotTo(o.HaveOccurred())
		if len(nodeList.Items) < 2 {
			g.Skip("Not enough node available, need at least two nodes for the test, skip the case!!")
		}

		exutil.By("2. Get namespace ")
		ns := oc.Namespace()

		exutil.By("3. Create a hello pod in ns")
		pod := pingPodResourceNode{
			name:      "hello-pod",
			namespace: ns,
			nodename:  nodeList.Items[0].Name,
			template:  pingPodNodeTemplate,
		}
		pod.createPingPodNode(oc)
		waitPodReady(oc, pod.namespace, pod.name)

		exutil.By("4. Create a nodePort type service fronting the above pod")
		svc := genericServiceResource{
			servicename:           "test-service",
			namespace:             ns,
			protocol:              "TCP",
			selector:              "hello-pod",
			serviceType:           "NodePort",
			ipFamilyPolicy:        "",
			internalTrafficPolicy: "Cluster",
			externalTrafficPolicy: "", //This no value parameter will be ignored
			template:              genericServiceTemplate,
		}
		if ipStackType == "dualstack" {
			svc.ipFamilyPolicy = "PreferDualStack"
		} else {
			svc.ipFamilyPolicy = "SingleStack"
		}
		defer removeResource(oc, true, true, "service", svc.servicename, "-n", svc.namespace)
		svc.createServiceFromParams(oc)
		exutil.By("5. Get NodePort at which service listens.")
		nodePort, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("service", "-n", ns, svc.servicename, "-o=jsonpath={.spec.ports[*].nodePort}").Output()
		o.Expect(err).NotTo(o.HaveOccurred())

		exutil.By("6. Validate external traffic to node port is redirected.")
		CurlNodePortPass(oc, nodeList.Items[1].Name, nodeList.Items[0].Name, nodePort)
		curlCmd := fmt.Sprintf("curl -4 -v http://www.google.de:%s --connect-timeout 5", nodePort)
		resp, err := exutil.DebugNodeWithChroot(oc, nodeList.Items[1].Name, "/bin/bash", "-c", curlCmd)
		if (err != nil) || (resp != "") {
			o.Expect(strings.Contains(resp, "Hello OpenShift")).To(o.BeFalse())
		}
	})
	//asood@redhat.com
	g.It("NonPreRelease-Longduration-Author:asood-Critical-63301-[FdpOvnOvs] Kube's API intermitent timeout via sdn or internal services from nodes or pods using hostnetwork. [Disruptive]", func() {
		// From customer bug https://issues.redhat.com/browse/OCPBUGS-5828
		var (
			buildPruningBaseDir    = exutil.FixturePath("testdata", "networking")
			hostNetworkPodTemplate = filepath.Join(buildPruningBaseDir, "ping-for-pod-hostnetwork-specific-node-template.yaml")
			pingPodNodeTemplate    = filepath.Join(buildPruningBaseDir, "ping-for-pod-specific-node-template.yaml")
			genericServiceTemplate = filepath.Join(buildPruningBaseDir, "service-generic-template.yaml")
		)
		//The test can run on the platforms that have nodes in same subnet as the hostnetworked pod backed service is accessible only such clusters.
		//The test also adds a bad route on the node from where the service is accessed for testing purpose.
		exutil.By("Check the platform if it is suitable for running the test")
		platform := exutil.CheckPlatform(oc)
		ipStackType := checkIPStackType(oc)
		if !strings.Contains(platform, "vsphere") && !strings.Contains(platform, "baremetal") {
			g.Skip("Unsupported platform, skipping the test")
		}
		if !strings.Contains(ipStackType, "ipv4single") {
			g.Skip("Unsupported stack, skipping the test")
		}
		exutil.By("Get the schedulable worker nodes in ready state")
		nodeList, nodeErr := e2enode.GetReadySchedulableNodes(context.TODO(), oc.KubeFramework().ClientSet)
		o.Expect(nodeErr).NotTo(o.HaveOccurred())
		if len(nodeList.Items) < 2 {
			g.Skip("This test requires at least two worker nodes")
		}

		exutil.By("Switch the GW mode to Local")
		origMode := getOVNGatewayMode(oc)
		desiredMode := "local"
		defer switchOVNGatewayMode(oc, origMode)
		switchOVNGatewayMode(oc, desiredMode)

		exutil.By("Get namespace ")
		ns := oc.Namespace()

		err := oc.AsAdmin().WithoutNamespace().Run("adm").Args("policy", "add-scc-to-group", "privileged", "system:serviceaccounts:"+ns).Execute()
		o.Expect(err).NotTo(o.HaveOccurred())
		exutil.By("Create pod on host network in namespace")
		pod1 := pingPodResourceNode{
			name:      "hello-pod",
			namespace: ns,
			nodename:  nodeList.Items[0].Name,
			template:  hostNetworkPodTemplate,
		}
		pod1.createPingPodNode(oc)
		waitPodReady(oc, ns, pod1.name)
		exutil.By("Create a test service which is in front of the above pods")
		svc := genericServiceResource{
			servicename:           "test-service-63301",
			namespace:             ns,
			protocol:              "TCP",
			selector:              "hello-pod",
			serviceType:           "ClusterIP",
			ipFamilyPolicy:        "SingleStack",
			internalTrafficPolicy: "Cluster",
			externalTrafficPolicy: "", //This no value parameter will be ignored
			template:              genericServiceTemplate,
		}

		svc.createServiceFromParams(oc)

		exutil.By("Check service status")
		svcOutput, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("service", "-n", ns, svc.servicename).Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(svcOutput).Should(o.ContainSubstring(svc.servicename))

		exutil.By("Get service IP")
		//nodeIP1 and nodeIP2 will be IPv6 and IPv4 respectively in case of dual stack and IPv4/IPv6 in 2nd var case of single
		_, nodeIP := getNodeIP(oc, nodeList.Items[0].Name)
		var curlCmd, addRouteCmd, delRouteCmd string

		svcIPv4 := getSvcIPv4(oc, svc.namespace, svc.servicename)
		curlCmd = fmt.Sprintf("curl -v %s:27017 --connect-timeout 5", svcIPv4)
		addRouteCmd = fmt.Sprintf("route add %s gw 127.0.0.1 lo", nodeIP)
		delRouteCmd = fmt.Sprintf("route delete %s", nodeIP)

		exutil.By("Create another pod for pinging the service")
		pod2 := pingPodResourceNode{
			name:      "ping-hello-pod",
			namespace: ns,
			nodename:  nodeList.Items[1].Name,
			template:  pingPodNodeTemplate,
		}
		pod2.createPingPodNode(oc)
		waitPodReady(oc, pod2.namespace, pod2.name)
		exutil.LabelPod(oc, pod2.namespace, pod2.name, "name-")
		exutil.LabelPod(oc, pod2.namespace, pod2.name, "name=ping-hello-pod")

		exutil.By("Validate the service from pod on cluster network")
		CurlPod2SvcPass(oc, ns, ns, pod2.name, svc.servicename)

		exutil.By("Validate the service from pod on host network")
		output, err := exutil.DebugNodeWithChroot(oc, nodeList.Items[1].Name, "bash", "-c", curlCmd)
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(strings.Contains(output, "Hello OpenShift!")).To(o.BeTrue())

		exutil.By("Create a bad route to node where pod backing the service is running, on the host from where service is accessed ")

		defer exutil.DebugNodeWithChroot(oc, nodeList.Items[1].Name, "/bin/bash", "-c", delRouteCmd)
		_, err = exutil.DebugNodeWithChroot(oc, nodeList.Items[1].Name, "/bin/bash", "-c", addRouteCmd)
		o.Expect(err).NotTo(o.HaveOccurred())

		exutil.By("Validate the service from pod on cluster network to verify it fails")
		CurlPod2SvcFail(oc, ns, ns, pod2.name, svc.servicename)

		exutil.By("Validate the service from pod on host network to verify it fails")
		output, err = exutil.DebugNodeWithChroot(oc, nodeList.Items[1].Name, "bash", "-c", curlCmd)
		if (err != nil) || (output != "") {
			o.Expect(strings.Contains(output, "Hello OpenShift!")).To(o.BeFalse())
		}
		exutil.By("Delete the route that was added")
		_, err = exutil.DebugNodeWithChroot(oc, nodeList.Items[1].Name, "/bin/bash", "-c", delRouteCmd)
		o.Expect(err).NotTo(o.HaveOccurred())
	})

	// author: jechen@redhat.com
	g.It("Author:jechen-High-71385-OVNK only choose LB endpoints from ready pods unless there are only terminating pods still in serving state left to choose.", func() {

		// For customer bug https://issues.redhat.com/browse/OCPBUGS-24363
		// OVNK choose LB endpoints in the following sequence:
		// 1. when there is/are pods in Ready state, ovnk ONLY choose endpoints of ready pods
		// 2. When there is/are no ready pods, ovnk choose endpoints that terminating + serving endpoints

		buildPruningBaseDir := exutil.FixturePath("testdata", "networking")
		testPodFile := filepath.Join(buildPruningBaseDir, "testpod-with-special-lifecycle.yaml")
		genericServiceTemplate := filepath.Join(buildPruningBaseDir, "service-generic-template.yaml")

		exutil.By("1.Get namespace \n")
		ns := oc.Namespace()

		exutil.By("2. Create test pods and scale test pods to 5 \n")
		createResourceFromFile(oc, ns, testPodFile)
		err := oc.AsAdmin().WithoutNamespace().Run("scale").Args("rc", "test-rc", "--replicas=5", "-n", ns).Execute()
		o.Expect(err).NotTo(o.HaveOccurred())
		err = waitForPodWithLabelReady(oc, ns, "name=test-pods")
		exutil.AssertWaitPollNoErr(err, "Not all test pods with label name=test-pods are ready")

		exutil.By("3. Create a service in front of the above test pods \n")
		svc := genericServiceResource{
			servicename:           "test-service",
			namespace:             ns,
			protocol:              "TCP",
			selector:              "test-pods",
			serviceType:           "ClusterIP",
			ipFamilyPolicy:        "",
			internalTrafficPolicy: "Cluster",
			externalTrafficPolicy: "", //This no value parameter will be ignored
			template:              genericServiceTemplate,
		}

		ipStack := checkIPStackType(oc)
		if ipStack == "dualstack" {
			svc.ipFamilyPolicy = "PreferDualStack"
		} else {
			svc.ipFamilyPolicy = "SingleStack"
		}
		svc.createServiceFromParams(oc)

		exutil.By("4. Check OVN service lb status \n")
		svcOutput, svcErr := oc.AsAdmin().WithoutNamespace().Run("get").Args("service", "-n", ns, svc.servicename).Output()
		o.Expect(svcErr).NotTo(o.HaveOccurred())
		o.Expect(svcOutput).Should(o.ContainSubstring(svc.servicename))

		exutil.By("5. Get IP for the OVN service lb \n")
		var svcIPv6, svcIPv4, podIPv6, podIPv4 string
		if ipStack == "dualstack" || ipStack == "ipv6single" {
			svcIPv6, svcIPv4 = getSvcIP(oc, svc.namespace, svc.servicename)
		} else {
			svcIPv4, _ = getSvcIP(oc, svc.namespace, svc.servicename)
		}
		e2e.Logf("On this %s cluster, IP for service IP are svcIPv6: %s,  svcIPv4: %s", ipStack, svcIPv6, svcIPv4)

		exutil.By("6. Check OVN service lb endpoints in northdb, it should include all running backend test pods \n")
		allPods, getPodErr := exutil.GetAllPodsWithLabel(oc, ns, "name=test-pods")
		o.Expect(getPodErr).NotTo(o.HaveOccurred())
		o.Expect(len(allPods)).NotTo(o.BeEquivalentTo(0))

		var expectedEndpointsv6, expectedEndpointsv4 []string
		for _, eachPod := range allPods {
			if ipStack == "dualstack" {
				podIPv6, podIPv4 = getPodIP(oc, ns, eachPod)
				expectedEndpointsv6 = append(expectedEndpointsv6, "["+podIPv6+"]:8080")
				expectedEndpointsv4 = append(expectedEndpointsv4, podIPv4+":8080")
			} else if ipStack == "ipv6single" {
				podIPv6, _ = getPodIP(oc, ns, eachPod)
				expectedEndpointsv6 = append(expectedEndpointsv6, "["+podIPv6+"]:8080")
			} else {
				podIPv4, _ = getPodIP(oc, ns, eachPod)
				expectedEndpointsv4 = append(expectedEndpointsv4, podIPv4+":8080")
			}
		}
		e2e.Logf("\n On this %s cluster, V6 endpoints of service lb are expected to be: %v\n", ipStack, expectedEndpointsv6)
		e2e.Logf("\n On this %s cluster, V4 endpoints of service lb are expected to be: %v\n", ipStack, expectedEndpointsv4)

		// check service lb endpoints in northdb on each node's ovnkube-pod
		nodeList, nodeErr := exutil.GetAllNodesbyOSType(oc, "linux")
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(nodeErr).NotTo(o.HaveOccurred())
		o.Expect(len(nodeList)).NotTo(o.BeEquivalentTo(0))

		var endpointsv6, endpointsv4 []string
		var epErr error
		for _, eachNode := range nodeList {
			if ipStack == "dualstack" || ipStack == "ipv6single" {
				endpointsv6, epErr = getLBListEndpointsbySVCIPPortinNBDB(oc, eachNode, "\\["+svcIPv6+"\\]:27017")
				e2e.Logf("\n Got V6 endpoints of service lb for node %s : %v\n", eachNode, expectedEndpointsv6)
				o.Expect(epErr).NotTo(o.HaveOccurred())
				o.Expect(unorderedEqual(endpointsv6, expectedEndpointsv6)).Should(o.BeTrue(), fmt.Sprintf("V6 service lb endpoints on node %sdo not match expected endpoints!", eachNode))
			}
			if ipStack == "dualstack" || ipStack == "ipv4single" {
				endpointsv4, epErr = getLBListEndpointsbySVCIPPortinNBDB(oc, eachNode, svcIPv4+":27017")
				e2e.Logf("\n Got V4 endpoints of service lb for node %s : %v\n", eachNode, expectedEndpointsv4)
				o.Expect(epErr).NotTo(o.HaveOccurred())
				o.Expect(unorderedEqual(endpointsv4, expectedEndpointsv4)).Should(o.BeTrue(), fmt.Sprintf("V4 service lb endpoints on node %sdo not match expected endpoints!", eachNode))
			}
		}

		exutil.By("7. Scale test pods down to 2 \n")
		scaleErr := oc.AsAdmin().WithoutNamespace().Run("scale").Args("rc", "test-rc", "--replicas=2", "-n", ns).Execute()
		o.Expect(scaleErr).NotTo(o.HaveOccurred())

		var terminatingPods []string
		o.Eventually(func() bool {
			terminatingPods = getAllPodsWithLabelAndCertainState(oc, ns, "name=test-pods", "Terminating")
			return len(terminatingPods) == 3
		}, "30s", "5s").Should(o.BeTrue(), "Test pods did not scale down to 2")
		e2e.Logf("\n terminatingPods: %v\n", terminatingPods)

		var expectedCleanedUpEPsv6, expectedCleanedUpEPsv4, expectedRemindedEPsv6, expectedRemindedEPsv4, actualFinalEPsv6, actualFinalEPsv4 []string
		for _, eachPod := range terminatingPods {
			if ipStack == "dualstack" {
				podIPv6, podIPv4 = getPodIP(oc, ns, eachPod)
				expectedCleanedUpEPsv6 = append(expectedCleanedUpEPsv6, "["+podIPv6+"]:8080")
				expectedCleanedUpEPsv4 = append(expectedCleanedUpEPsv4, podIPv4+":8080")
			} else if ipStack == "ipv6single" {
				podIPv6, _ = getPodIP(oc, ns, eachPod)
				expectedCleanedUpEPsv6 = append(expectedCleanedUpEPsv6, "["+podIPv6+"]:8080")
			} else {
				podIPv4, _ = getPodIP(oc, ns, eachPod)
				expectedCleanedUpEPsv4 = append(expectedCleanedUpEPsv4, podIPv4+":8080")
			}
		}
		e2e.Logf("\n On this %s cluster, V6 endpoints of service lb are expected to be cleaned up: %v\n", ipStack, expectedCleanedUpEPsv6)
		e2e.Logf("\n On this %s cluster, V4 endpoints of service lb are expected to be cleaned up: %v\n", ipStack, expectedCleanedUpEPsv4)

		runningPods := getAllPodsWithLabelAndCertainState(oc, ns, "name=test-pods", "Running")
		o.Expect(len(runningPods)).To(o.BeEquivalentTo(2))
		e2e.Logf("\n runningPods: %v\n", runningPods)

		for _, eachPod := range runningPods {
			if ipStack == "dualstack" {
				podIPv6, podIPv4 = getPodIP(oc, ns, eachPod)
				expectedRemindedEPsv6 = append(expectedRemindedEPsv6, "["+podIPv6+"]:8080")
				expectedRemindedEPsv4 = append(expectedRemindedEPsv4, podIPv4+":8080")
			} else if ipStack == "ipv6single" {
				podIPv6, _ = getPodIP(oc, ns, eachPod)
				expectedRemindedEPsv6 = append(expectedRemindedEPsv6, "["+podIPv6+"]:8080")
			} else {
				podIPv4, _ = getPodIP(oc, ns, eachPod)
				expectedRemindedEPsv4 = append(expectedRemindedEPsv4, podIPv4+":8080")
			}
		}
		e2e.Logf("\n On this %s cluster, V6 endpoints of service lb are expected to remind: %v\n", ipStack, expectedRemindedEPsv6)
		e2e.Logf("\n On this %s cluster, V4 endpoints of service lb are expected to remind: %v\n", ipStack, expectedRemindedEPsv4)

		exutil.By("8. Check lb-list entries in northdb again in each node's ovnkube-node pod, only Ready pods' endpoints reminded in service lb endpoints \n")
		for _, eachNode := range nodeList {
			if ipStack == "dualstack" || ipStack == "ipv6single" {
				actualFinalEPsv6, epErr = getLBListEndpointsbySVCIPPortinNBDB(oc, eachNode, "\\["+svcIPv6+"\\]:27017")
				e2e.Logf("\n\n After scale-down to 2, V6 endpoints from lb-list output on node %s northdb: %v\n\n", eachNode, actualFinalEPsv6)
				o.Expect(epErr).NotTo(o.HaveOccurred())
				o.Expect(unorderedEqual(actualFinalEPsv6, expectedRemindedEPsv6)).Should(o.BeTrue(), fmt.Sprintf("After scale-down, V6 service lb endpoints on node %s do not match expected endpoints!", eachNode))
			}
			if ipStack == "dualstack" || ipStack == "ipv4single" {
				actualFinalEPsv4, epErr = getLBListEndpointsbySVCIPPortinNBDB(oc, eachNode, svcIPv4+":27017")
				o.Expect(epErr).NotTo(o.HaveOccurred())
				e2e.Logf("\n\n After scale-down to 2, V4 endpoints from lb-list output on node %s northdb: %v\n\n", eachNode, actualFinalEPsv4)
				o.Expect(unorderedEqual(actualFinalEPsv4, expectedRemindedEPsv4)).Should(o.BeTrue(), fmt.Sprintf("After scale-down, V4 service lb endpoints on node %s do not match expected endpoints!", eachNode))
			}
			// Verify terminating pods' endpoints are not in final service lb endpoints
			if ipStack == "dualstack" || ipStack == "ipv6single" {
				for _, ep := range expectedCleanedUpEPsv6 {
					o.Expect(isValueInList(ep, actualFinalEPsv6)).ShouldNot(o.BeTrue(), fmt.Sprintf("After scale-down, terminating pod's V6 endpoint %s is not cleaned up from V6 service lb endpoint", ep))
				}
			}
			if ipStack == "dualstack" || ipStack == "ipv4single" {
				for _, ep := range expectedCleanedUpEPsv4 {
					o.Expect(isValueInList(ep, actualFinalEPsv4)).ShouldNot(o.BeTrue(), fmt.Sprintf("After scale-down, terminating pod's V4 endpoint %s is not cleaned up from V4 service lb endpoint", ep))
				}
			}
		}

		exutil.By("9. Wait for all three terminating pods from step 7-8 to disappear so that only two running pods are left\n")
		o.Eventually(func() bool {
			allPodsWithLabel := getPodName(oc, ns, "name=test-pods")
			runningPods = getAllPodsWithLabelAndCertainState(oc, ns, "name=test-pods", "Running")
			return len(runningPods) == len(allPodsWithLabel)
		}, "180s", "10s").Should(o.BeTrue(), "Terminating pods did not disappear after waiting enough time")

		exutil.By("10. Scale test pods down to 0 \n")
		scaleErr = oc.AsAdmin().WithoutNamespace().Run("scale").Args("rc", "test-rc", "--replicas=0", "-n", ns).Execute()
		o.Expect(scaleErr).NotTo(o.HaveOccurred())

		o.Eventually(func() bool {
			terminatingPods = getAllPodsWithLabelAndCertainState(oc, ns, "name=test-pods", "Terminating")
			return len(terminatingPods) == 2
		}, "30s", "5s").Should(o.BeTrue(), "Test pods did not scale down to 0")
		e2e.Logf("\n terminatingPods: %v\n", terminatingPods)

		exutil.By("11. Check lb-list entries in northdb again in each node's ovnkube-node pod, verify that the two terminating but serving pods reminded in service lb endpoints \n")

		// expectedRemindedEPv4 or expectedRemindedEPv6 or both are still expected in NBDB for a little while,
		// that is because these two pods transition from Running state to terminating but serving state and there is no other running pod available
		for _, eachNode := range nodeList {
			if ipStack == "dualstack" || ipStack == "ipv6single" {
				actualFinalEPsv6, epErr = getLBListEndpointsbySVCIPPortinNBDB(oc, eachNode, "\\["+svcIPv6+"\\]:27017")
				e2e.Logf("\n\n After scale-down to 0, V6 endpoints from lb-list output on node %s northdb: %v\n\n", eachNode, actualFinalEPsv6)
				o.Expect(epErr).NotTo(o.HaveOccurred())
				o.Expect(unorderedEqual(actualFinalEPsv6, expectedRemindedEPsv6)).Should(o.BeTrue(), fmt.Sprintf("After scale-down, V6 service lb endpoints on node %s do not match expected endpoints!", eachNode))
			}
			if ipStack == "dualstack" || ipStack == "ipv4single" {
				actualFinalEPsv4, epErr = getLBListEndpointsbySVCIPPortinNBDB(oc, eachNode, svcIPv4+":27017")
				o.Expect(epErr).NotTo(o.HaveOccurred())
				e2e.Logf("\n\n After scale-down to 0, V4 endpoints from lb-list output on node %s northdb: %v\n\n", eachNode, actualFinalEPsv4)
				o.Expect(unorderedEqual(actualFinalEPsv4, expectedRemindedEPsv4)).Should(o.BeTrue(), fmt.Sprintf("After scale-down, V4 service lb endpoints on node %s do not match expected endpoints!", eachNode))
			}
		}
	})

	// author: jechen@redhat.com
	g.It("Author:jechen-High-37033-ExternalVM access cluster through externalIP. [Disruptive]", func() {

		// This is for https://bugzilla.redhat.com/show_bug.cgi?id=1900118 and https://bugzilla.redhat.com/show_bug.cgi?id=1890270

		buildPruningBaseDir := exutil.FixturePath("testdata", "networking")
		externalIPServiceTemplate := filepath.Join(buildPruningBaseDir, "externalip_service1-template.yaml")
		externalIPPodTemplate := filepath.Join(buildPruningBaseDir, "externalip_pod-template.yaml")
		var workers, nonExternalIPNodes []string
		var proxyHost, RDUHost, intf string

		if !(isPlatformSuitable(oc)) {
			g.Skip("These cases can only be run on networking team's private RDU clusters, skip for other envrionment!!!")
		}
		workers = excludeSriovNodes(oc)
		if len(workers) < 2 {
			g.Skip("Not enough nodes, need minimal 2 nodes on RDU for the test, skip the case!!")
		}
		msg, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("routes", "console", "-n", "openshift-console").Output()
		if err != nil || strings.Contains(msg, "sriov.openshift-qe.sdn.com") {
			proxyHost = "10.8.1.181"
			RDUHost = "openshift-qe-028.lab.eng.rdu2.redhat.com"
			intf = "sriovbm"
		}
		if err != nil || strings.Contains(msg, "offload.openshift-qe.sdn.com") {
			proxyHost = "10.8.1.179"
			RDUHost = "openshift-qe-026.lab.eng.rdu2.redhat.com"
			intf = "offloadbm"
		}

		exutil.By("1. Get namespace, create an externalIP pod in it\n")
		ns := oc.Namespace()
		pod1 := externalIPPod{
			name:      "externalip-pod",
			namespace: ns,
			template:  externalIPPodTemplate,
		}
		defer removeResource(oc, true, true, "pod", pod1.name, "-n", pod1.namespace)
		pod1.createExternalIPPod(oc)
		waitPodReady(oc, pod1.namespace, pod1.name)

		exutil.By("2.Find another node, get its host CIDR, and one unused IP in its subnet \n")
		externalIPPodNode, err := exutil.GetPodNodeName(oc, pod1.namespace, pod1.name)
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(externalIPPodNode).NotTo(o.Equal(""))
		e2e.Logf("ExternalIP pod is on node: %s", externalIPPodNode)

		for _, node := range workers {
			if node != externalIPPodNode {
				nonExternalIPNodes = append(nonExternalIPNodes, node)
			}
		}
		e2e.Logf("\n nonExternalIPNodes are: %v\n", nonExternalIPNodes)

		sub := getEgressCIDRsForNode(oc, nonExternalIPNodes[0])
		freeIPs := findUnUsedIPsOnNodeOrFail(oc, nonExternalIPNodes[0], sub, 1)
		o.Expect(len(freeIPs)).Should(o.Equal(1))

		exutil.By("4.Patch update network.config with the host CIDR to enable externalIP \n")
		defer patchResourceAsAdmin(oc, "network/cluster", "{\"spec\":{\"externalIP\":{\"policy\":{}}}}")
		defer patchResourceAsAdmin(oc, "network/cluster", "{\"spec\":{\"externalIP\":{\"policy\":{\"allowedCIDRs\":[]}}}}")
		patchResourceAsAdmin(oc, "network/cluster", "{\"spec\":{\"externalIP\":{\"policy\":{\"allowedCIDRs\":[\""+sub+"\"]}}}}")

		exutil.By("5.Create an externalIP service with the unused IP address obtained above as externalIP\n")
		svc := externalIPService{
			name:       "service-unsecure",
			namespace:  ns,
			externalIP: freeIPs[0],
			template:   externalIPServiceTemplate,
		}
		defer removeResource(oc, true, true, "service", svc.name, "-n", svc.namespace)
		parameters := []string{"--ignore-unknown-parameters=true", "-f", svc.template, "-p", "NAME=" + svc.name, "EXTERNALIP=" + svc.externalIP}
		exutil.ApplyNsResourceFromTemplate(oc, svc.namespace, parameters...)
		svcOutput, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("service", "-n", ns, svc.name).Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(svcOutput).Should(o.ContainSubstring(svc.name))

		g.By("Get the Node IP from any node, add a static route on the test runner host to assist the test")
		nodeIP := getNodeIPv4(oc, ns, nonExternalIPNodes[0])
		ipRouteDeleteCmd := "ip route delete " + svc.externalIP
		defer sshRunCmd(RDUHost, "root", ipRouteDeleteCmd)
		ipRouteAddCmd := "ip route add " + svc.externalIP + " via " + nodeIP + " dev " + intf
		err = sshRunCmd(proxyHost, "root", ipRouteAddCmd)
		o.Expect(err).NotTo(o.HaveOccurred())

		exutil.By("6.Validate the externalIP service from external of the cluster (from test runner)\n")
		svc4URL := net.JoinHostPort(svc.externalIP, "27017")
		svcChkCmd := fmt.Sprintf("curl -H 'Cache-Control: no-cache' -x 'http://%s:8888' %s --connect-timeout 5", proxyHost, svc4URL)
		e2e.Logf("\n svcChkCmd: %v\n", svcChkCmd)
		output, curlErr := exec.Command("bash", "-c", svcChkCmd).Output()
		o.Expect(curlErr).NotTo(o.HaveOccurred())
		o.Expect(strings.Contains(string(output), "Hello OpenShift")).Should(o.BeTrue(), "The externalIP service is not reachable as expected")
	})

	// author: jechen@redhat.com
	g.It("Author:jechen-High-43492-ExternalIP for node that has secondary IP. [Disruptive]", func() {

		// This is for bug https://bugzilla.redhat.com/show_bug.cgi?id=1959798

		buildPruningBaseDir := exutil.FixturePath("testdata", "networking")
		externalIPServiceTemplate := filepath.Join(buildPruningBaseDir, "externalip_service1-template.yaml")
		externalIPPodTemplate := filepath.Join(buildPruningBaseDir, "externalip_pod-template.yaml")
		intf := "br-ex"
		var workers, nonExternalIPNodes []string
		var proxyHost string

		networkType := exutil.CheckNetworkType(oc)
		if !strings.Contains(networkType, "ovn") {
			g.Skip("Incompatible networkType, skipping test!!!")
		}

		platform := exutil.CheckPlatform(oc)
		msg, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("routes", "console", "-n", "openshift-console").Output()
		if err != nil || strings.Contains(msg, "sriov.openshift-qe.sdn.com") {
			platform = "rdu1"
			proxyHost = "10.8.1.181"
		}
		if err != nil || strings.Contains(msg, "offload.openshift-qe.sdn.com") {
			platform = "rdu2"
			proxyHost = "10.8.1.179"
		}

		if strings.Contains(platform, "rdu1") || strings.Contains(platform, "rdu2") {
			workers = excludeSriovNodes(oc)
			if len(workers) < 2 {
				g.Skip("Not enough nodes, need minimal 2 nodes on RDU for the test, skip the case!!")
			}
		} else {
			nodeList, err := e2enode.GetReadySchedulableNodes(context.TODO(), oc.KubeFramework().ClientSet)
			o.Expect(err).NotTo(o.HaveOccurred())

			// for other non-RDU platforms, need minimal 3 nodes for the test
			if len(nodeList.Items) < 3 {
				g.Skip("Not enough worker nodes for this test, skip the case!!")
			}
			for _, node := range nodeList.Items {
				workers = append(workers, node.Name)
			}
		}

		exutil.By("1. Get namespace, create an externalIP pod in it\n")
		ns := oc.Namespace()
		pod1 := externalIPPod{
			name:      "externalip-pod",
			namespace: ns,
			template:  externalIPPodTemplate,
		}
		pod1.createExternalIPPod(oc)
		waitPodReady(oc, pod1.namespace, pod1.name)

		exutil.By("2.Find another node, get its host CIDR, and one unused IP in its subnet \n")
		externalIPPodNode, err := exutil.GetPodNodeName(oc, pod1.namespace, pod1.name)
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(externalIPPodNode).NotTo(o.Equal(""))
		e2e.Logf("ExternalIP pod is on node: %s", externalIPPodNode)

		for _, node := range workers {
			if node != externalIPPodNode {
				nonExternalIPNodes = append(nonExternalIPNodes, node)
			}
		}
		e2e.Logf("\n nonExternalIPNodes are: %v\n", nonExternalIPNodes)

		sub := getEgressCIDRsForNode(oc, nonExternalIPNodes[0])
		freeIPs := findUnUsedIPsOnNodeOrFail(oc, nonExternalIPNodes[0], sub, 1)
		o.Expect(len(freeIPs)).Should(o.Equal(1))
		_, hostIPwithPrefix := getIPv4AndIPWithPrefixForNICOnNode(oc, nonExternalIPNodes[0], intf)
		prefix := strings.Split(hostIPwithPrefix, "/")[1]
		e2e.Logf("\n On host %s, prefix of the host ip address: %v\n", nonExternalIPNodes[0], prefix)

		exutil.By(fmt.Sprintf("3. Add secondary IP %s to br-ex on the node %s", freeIPs[0]+"/"+prefix, nonExternalIPNodes[0]))
		defer delIPFromInferface(oc, nonExternalIPNodes[0], freeIPs[0], intf)
		addIPtoInferface(oc, nonExternalIPNodes[0], freeIPs[0]+"/"+prefix, intf)

		exutil.By("4.Patch update network.config with the host CIDR to enable externalIP \n")
		original, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("network/cluster", "-ojsonpath={.spec.externalIP}").Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		patch := `[{"op": "replace", "path": "/spec/externalIP", "value": ` + original + `}]`
		defer oc.AsAdmin().WithoutNamespace().Run("patch").Args("network/cluster", "-p", patch, "--type=json").Execute()
		patchResourceAsAdmin(oc, "network/cluster", "{\"spec\":{\"externalIP\":{\"policy\":{\"allowedCIDRs\":[\""+sub+"\"]}}}}")

		exutil.By("5.Create an externalIP service with the unused IP address obtained above as externalIP\n")
		svc := externalIPService{
			name:       "service-unsecure",
			namespace:  ns,
			externalIP: freeIPs[0],
			template:   externalIPServiceTemplate,
		}
		defer removeResource(oc, true, true, "service", svc.name, "-n", svc.namespace)
		parameters := []string{"--ignore-unknown-parameters=true", "-f", svc.template, "-p", "NAME=" + svc.name, "EXTERNALIP=" + svc.externalIP}
		exutil.ApplyNsResourceFromTemplate(oc, svc.namespace, parameters...)
		svcOutput, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("service", "-n", ns, svc.name).Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(svcOutput).Should(o.ContainSubstring(svc.name))

		// For RDU, curl the externalIP service from test runner through proxy
		// For other platforms, since it is hard to get external host on same subnet of the secondary IP, we use another non-externalIP node as simulated test enviornment to validate
		exutil.By("6.Validate the externalIP service\n")
		svc4URL := net.JoinHostPort(svc.externalIP, "27017")
		var host string
		if platform == "rdu1" || platform == "rdu2" {
			exutil.By(fmt.Sprintf("On %s,  use test runner to validate the externalIP service", platform))
			host = proxyHost
		} else {
			exutil.By(fmt.Sprintf("On %s,  use another non-externalIP node to validate the externalIP service", platform))
			host = nonExternalIPNodes[1]
		}
		checkSvcErr := wait.Poll(10*time.Second, 2*time.Minute, func() (bool, error) {
			if validateService(oc, host, svc4URL) {
				return true, nil
			}
			return false, nil
		})
		exutil.AssertWaitPollNoErr(checkSvcErr, "The externalIP service is not reachable as expected")

		exutil.By("7.Check OVN-KUBE-EXTERNALIP iptables chain is updated correctly\n")
		for _, node := range workers {
			output, err := exutil.DebugNodeWithChroot(oc, node, "/bin/bash", "-c", "iptables -n -v -t nat -L OVN-KUBE-EXTERNALIP")
			o.Expect(err).NotTo(o.HaveOccurred())
			o.Expect(strings.Contains(output, svc.externalIP)).Should(o.BeTrue(), fmt.Sprintf("OVN-KUBE-EXTERNALIP iptables chain was not updated correctly on node %s", node))
		}
	})

	// author: jechen@redhat.com
	g.It("Author:jechen-High-24672-ExternalIP configured from autoAssignCIDRs. [Disruptive]", func() {

		buildPruningBaseDir := exutil.FixturePath("testdata", "networking")
		pingPodNodeTemplate := filepath.Join(buildPruningBaseDir, "ping-for-pod-specific-node-template.yaml")
		genericServiceTemplate := filepath.Join(buildPruningBaseDir, "service-generic-template.yaml")

		platform := exutil.CheckPlatform(oc)
		e2e.Logf("platform %s", platform)
		acceptedPlatform := strings.Contains(platform, "gcp") || strings.Contains(platform, "azure")
		if !acceptedPlatform {
			g.Skip("Test cases should be run on GCP, Azure, skip for other platforms or other network plugin!!")
		}
		// skip if no spec.publicZone specified in dns.config
		// the private cluster will be skipped as well
		// refer to https://issues.redhat.com/browse/OCPQE-22704
		dnsPublicZone, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("dns.config/cluster", "-ojsonpath={.spec.publicZone}").Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		if dnsPublicZone == "" {
			g.Skip("Skip for the platforms that no dns publicZone specified")
		}

		nodeList, err := e2enode.GetReadySchedulableNodes(context.TODO(), oc.KubeFramework().ClientSet)
		o.Expect(err).NotTo(o.HaveOccurred())

		if len(nodeList.Items) < 2 {
			g.Skip("Not enough nodes, need 2 nodes for the test, skip the case!!")
		}

		exutil.By("1. Get namespace\n")
		ns := oc.Namespace()

		svc := genericServiceResource{
			servicename:           "test-service",
			namespace:             ns,
			protocol:              "TCP",
			selector:              "hello-pod",
			serviceType:           "LoadBalancer",
			ipFamilyPolicy:        "SingleStack",
			internalTrafficPolicy: "Cluster",
			externalTrafficPolicy: "Cluster",
			template:              genericServiceTemplate,
		}

		// For GCP/Azure, create a loadbalancer service first to get LB service's LB ip address, then derive its subnet to be used in step 3,
		exutil.By("2. For public cloud platform, create a loadBalancer service first\n")
		svc.createServiceFromParams(oc)
		svcOutput, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("service", "-n", ns, svc.servicename).Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(svcOutput).Should(o.ContainSubstring(svc.servicename))

		exutil.By("3. Create a test pod\n")
		pod := pingPodResourceNode{
			name:      "hello-pod",
			namespace: ns,
			nodename:  nodeList.Items[0].Name,
			template:  pingPodNodeTemplate,
		}
		pod.createPingPodNode(oc)
		waitPodReady(oc, ns, pod.name)

		exutil.By("4. For GCP/Azure, get LB's ip address\n")
		svcExternalIP := getLBSVCIP(oc, svc.namespace, svc.servicename)
		e2e.Logf("Got externalIP service IP: %v", svcExternalIP)
		o.Expect(svcExternalIP).NotTo(o.BeEmpty())

		exutil.By("5. Derive LB's subnet from its IP address\n")
		ingressLBIP := net.ParseIP(svcExternalIP)
		if ingressLBIP == nil {
			g.Skip("Did not get valid IP address for the host of LB service, skip the rest of test!!")
		}
		mask := net.CIDRMask(24, 32) // Assuming /24 subnet mask
		subnet := ingressLBIP.Mask(mask).String() + "/24"
		e2e.Logf("LB's subnet: %v", subnet)

		exutil.By("6. Patch update network.config with subnet obtained above to enable autoAssignCIDR for externalIP\n")
		original, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("network/cluster", "-ojsonpath={.spec.externalIP}").Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		patch := `[{"op": "replace", "path": "/spec/externalIP", "value": ` + original + `}]`
		defer oc.AsAdmin().WithoutNamespace().Run("patch").Args("network/cluster", "-p", patch, "--type=json").Execute()
		patchResourceAsAdmin(oc, "network/cluster", "{\"spec\":{\"externalIP\":{\"autoAssignCIDRs\":[\""+subnet+"\"]}}}")
		patchResourceAsAdmin(oc, "network/cluster", "{\"spec\":{\"externalIP\":{\"policy\":{\"allowedCIDRs\":[\""+subnet+"\"]}}}}")

		// Wait a little for autoAssignCIDR to take effect
		time.Sleep(10 * time.Second)

		exutil.By("7.Curl the externalIP service from test runner\n")
		svc4URL := net.JoinHostPort(svcExternalIP, "27017")
		svcChkCmd := fmt.Sprintf("curl  %s --connect-timeout 30", svc4URL)
		e2e.Logf("\n svcChkCmd: %v\n", svcChkCmd)
		output, err := exec.Command("bash", "-c", svcChkCmd).Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(strings.Contains(string(output), "Hello OpenShift")).Should(o.BeTrue(), "The externalIP service is not reachable as expected")
	})

	// author: jechen@redhat.com
	g.It("Author:jechen-High-74601-Verify traffic and OVNK LB endpoints in nbdb for LoadBalancer Service when externalTrafficPolicy is set to Cluster.[Serial]", func() {

		// For customer bug https://issues.redhat.com/browse/OCPBUGS-24363
		// OVNK choose LB endpoints in the following sequence:
		// 1. when there is/are pods in Ready state, ovnk ONLY choose endpoints of ready pods
		// 2. When there is/are no ready pods, ovnk choose endpoints that terminating + serving endpoints

		buildPruningBaseDir := exutil.FixturePath("testdata", "networking")
		testPodFile := filepath.Join(buildPruningBaseDir, "testpod-with-special-lifecycle.yaml")
		genericServiceTemplate := filepath.Join(buildPruningBaseDir, "service-generic-template.yaml")

		platform := exutil.CheckPlatform(oc)
		networkType := checkNetworkType(oc)
		e2e.Logf("\n\nThe platform is %v,  networkType is %v\n", platform, networkType)
		scheduleableNodeList, err := e2enode.GetReadySchedulableNodes(context.TODO(), oc.KubeFramework().ClientSet)
		o.Expect(err).NotTo(o.HaveOccurred())
		acceptedPlatform := strings.Contains(platform, "gcp") || strings.Contains(platform, "azure")
		if !acceptedPlatform || !strings.Contains(networkType, "ovn") || len(scheduleableNodeList.Items) < 2 {
			g.Skip("Test cases should be run on GCP or Azure cluster with ovn network plugin, minimal 2 nodes are required, skip for others that do not meet the test requirement")
		}

		exutil.By("1. Get namespace, create 2 test pods in it, create a service in front of the test pods \n")
		ns := oc.Namespace()
		createResourceFromFile(oc, ns, testPodFile)
		err = waitForPodWithLabelReady(oc, ns, "name=test-pods")
		exutil.AssertWaitPollNoErr(err, "Not all test pods with label name=test-pods are ready")

		exutil.By("2. Create a service in front of the above test pods \n")
		svc := genericServiceResource{
			servicename:           "test-service",
			namespace:             ns,
			protocol:              "TCP",
			selector:              "test-pods",
			serviceType:           "LoadBalancer",
			ipFamilyPolicy:        "SingleStack",
			internalTrafficPolicy: "Cluster",
			externalTrafficPolicy: "Cluster",
			template:              genericServiceTemplate,
		}
		svc.createServiceFromParams(oc)
		svcOutput, err := oc.AsAdmin().WithoutNamespace().Run("get").Args("service", "-n", ns, svc.servicename).Output()
		o.Expect(err).NotTo(o.HaveOccurred())
		o.Expect(svcOutput).Should(o.ContainSubstring(svc.servicename))

		exutil.By("3. Get IP for the OVN service lb \n")
		var svcIPv4, podIPv4, curlSVC4ChkCmd string
		svcIPv4, _ = getSvcIP(oc, svc.namespace, svc.servicename)
		curlSVC4ChkCmd = fmt.Sprintf("for i in {1..10}; do curl %s --connect-timeout 5 ; sleep 2;echo ;done", net.JoinHostPort(svcIPv4, "27017"))
		e2e.Logf("IP for service IP: %s", svcIPv4)

		exutil.By("4. Before scale down test pods, check OVN service lb endpoints in northdb and traffic at endpoints \n")
		exutil.By("4.1. Check OVN service lb endpoints in northdb, it should include all running backend test pods \n")
		allPods, getPodErr := exutil.GetAllPodsWithLabel(oc, ns, "name=test-pods")
		o.Expect(getPodErr).NotTo(o.HaveOccurred())
		o.Expect(len(allPods)).NotTo(o.BeEquivalentTo(0))

		var expectedEndpointsv4 []string
		podNodeNames := make(map[string]string)
		podIPv4s := make(map[string]string)
		for _, eachPod := range allPods {
			nodeName, getNodeErr := exutil.GetPodNodeName(oc, ns, eachPod)
			o.Expect(getNodeErr).NotTo(o.HaveOccurred())
			podNodeNames[eachPod] = nodeName
			podIPv4, _ = getPodIP(oc, ns, eachPod)
			podIPv4s[eachPod] = podIPv4
			expectedEndpointsv4 = append(expectedEndpointsv4, podIPv4+":8080")

		}
		e2e.Logf("\n V4 endpoints of service lb are expected to be: %v\n", expectedEndpointsv4)

		// check service lb endpoints in northdb on each node's ovnkube-pod
		nodeList, nodeErr := exutil.GetAllNodesbyOSType(oc, "linux")
		o.Expect(nodeErr).NotTo(o.HaveOccurred())
		o.Expect(len(nodeList)).NotTo(o.BeEquivalentTo(0))

		var endpointsv4 []string
		var epErr error
		for _, eachNode := range nodeList {
			endpointsv4, epErr = getLBListEndpointsbySVCIPPortinNBDB(oc, eachNode, svcIPv4+":27017")
			e2e.Logf("\n Got V4 endpoints of service lb for node %s : %v\n", eachNode, expectedEndpointsv4)
			o.Expect(epErr).NotTo(o.HaveOccurred())
			o.Expect(unorderedEqual(endpointsv4, expectedEndpointsv4)).Should(o.BeTrue(), fmt.Sprintf("V4 service lb endpoints on node %sdo not match expected endpoints!", eachNode))

		}

		exutil.By("4.2. Verify all running pods get traffic \n")
		var channels [2]chan string
		// Initialize each channel in the array
		for i := range channels {
			channels[i] = make(chan string)
		}

		exutil.By(" Start tcpdump on each pod's node")
		for i, pod := range allPods {
			go func(i int, pod string) {
				defer g.GinkgoRecover()
				tcpdumpCmd := fmt.Sprintf(`timeout 60s tcpdump -c 4 -nneep -i any "(dst port 8080) and (dst %s)"`, podIPv4s[pod])
				outputTcpdump, _ := oc.AsAdmin().WithoutNamespace().Run("debug").Args("-n", "default", "node/"+podNodeNames[pod], "--", "bash", "-c", tcpdumpCmd).Output()
				channels[i] <- outputTcpdump
			}(i, pod)
		}
		// add sleep time to let the ping action happen later after tcpdump is enabled.
		time.Sleep(5 * time.Second)

		exutil.By(" Curl the externalIP service from test runner\n")
		output, curlErr := exec.Command("bash", "-c", curlSVC4ChkCmd).Output()
		o.Expect(curlErr).NotTo(o.HaveOccurred())

		for i, pod := range allPods {
			receivedMsg := <-channels[i]
			e2e.Logf(" at step 4.2, tcpdumpOutput for node %s is \n%s\n\n", podNodeNames[pod], receivedMsg)
			o.Expect(strings.Contains(receivedMsg, podIPv4s[pod])).Should(o.BeTrue())
		}
		o.Expect(strings.Contains(string(output), "Hello OpenShift")).Should(o.BeTrue(), "The externalIP service is not reachable as expected")

		exutil.By("5. Scale test pods down to 1 \n")
		scaleErr := oc.AsAdmin().WithoutNamespace().Run("scale").Args("rc", "test-rc", "--replicas=1", "-n", ns).Execute()
		o.Expect(scaleErr).NotTo(o.HaveOccurred())

		allPods = allPods[:0]
		var terminatingPods []string
		o.Eventually(func() bool {
			terminatingPods = getAllPodsWithLabelAndCertainState(oc, ns, "name=test-pods", "Terminating")
			return len(terminatingPods) == 1
		}, "30s", "5s").Should(o.BeTrue(), "Test pods did not scale down to 1")
		e2e.Logf("\n terminatingPods: %v\n", terminatingPods)
		allPods = append(allPods, terminatingPods[0])

		var expectedCleanedUpEPsv4, expectedRemindedEPsv4, actualFinalEPsv4 []string
		for _, eachPod := range terminatingPods {
			expectedCleanedUpEPsv4 = append(expectedCleanedUpEPsv4, podIPv4s[eachPod]+":8080")
		}
		e2e.Logf("\n V4 endpoints of service lb are expected to be cleaned up: %v\n", expectedCleanedUpEPsv4)

		runningPods := getAllPodsWithLabelAndCertainState(oc, ns, "name=test-pods", "Running")
		o.Expect(len(runningPods)).To(o.BeEquivalentTo(1))
		e2e.Logf("\n runningPods: %v\n", runningPods)
		allPods = append(allPods, runningPods[0])

		for _, eachPod := range runningPods {
			expectedRemindedEPsv4 = append(expectedRemindedEPsv4, podIPv4s[eachPod]+":8080")
		}
		e2e.Logf("\n V4 endpoints of service lb are expected to remind: %v\n", expectedRemindedEPsv4)

		exutil.By("5.1. Check lb-list entries in northdb again in each node's ovnkube-node pod, only Ready pods' endpoints reminded in service lb endpoints \n")
		for _, eachNode := range nodeList {
			actualFinalEPsv4, epErr = getLBListEndpointsbySVCIPPortinNBDB(oc, eachNode, svcIPv4+":27017")
			o.Expect(epErr).NotTo(o.HaveOccurred())
			e2e.Logf("\n\n After scale-down to 2, V4 endpoints from lb-list output on node %s northdb: %v\n\n", eachNode, actualFinalEPsv4)
			o.Expect(unorderedEqual(actualFinalEPsv4, expectedRemindedEPsv4)).Should(o.BeTrue(), fmt.Sprintf("After scale-down, V4 service lb endpoints on node %s do not match expected endpoints!", eachNode))

			// Verify terminating pods' endpoints are not in final service lb endpoints
			for _, ep := range expectedCleanedUpEPsv4 {
				o.Expect(isValueInList(ep, actualFinalEPsv4)).ShouldNot(o.BeTrue(), fmt.Sprintf("After scale-down, terminating pod's V4 endpoint %s is not cleaned up from V4 service lb endpoint", ep))
			}
		}

		exutil.By("5.2 Verify only the running pod receives traffic, the terminating pod does not receive traffic \n")
		for i, pod := range allPods {
			go func(i int, pod string) {
				defer g.GinkgoRecover()
				tcpdumpCmd := fmt.Sprintf(`timeout 60s tcpdump -c 4 -nneep -i any "(dst port 8080) and (dst %s)"`, podIPv4s[pod])
				outputTcpdump, _ := oc.AsAdmin().WithoutNamespace().Run("debug").Args("-n", "default", "node/"+podNodeNames[pod], "--", "bash", "-c", tcpdumpCmd).Output()
				channels[i] <- outputTcpdump
			}(i, pod)
		}

		// add sleep time to let the ping action happen later after tcpdump is enabled.
		time.Sleep(5 * time.Second)
		output, curlErr = exec.Command("bash", "-c", curlSVC4ChkCmd).Output()
		o.Expect(curlErr).NotTo(o.HaveOccurred())

		for i, pod := range allPods {
			receivedMsg := <-channels[i]
			e2e.Logf(" at step 5.2, tcpdumpOutput for node %s is \n%s\n\n", podNodeNames[pod], receivedMsg)
			if pod == terminatingPods[0] {
				o.Expect(strings.Contains(receivedMsg, "0 packets captured")).Should(o.BeTrue())
			} else {
				o.Expect(strings.Contains(receivedMsg, podIPv4s[pod])).Should(o.BeTrue())
			}
		}
		o.Expect(strings.Contains(string(output), "Hello OpenShift")).Should(o.BeTrue(), "The externalIP service is not reachable as expected")

		exutil.By("5.3. Wait for terminating pod from step 7 to disappear so that there is only one running pod left\n")
		o.Eventually(func() bool {
			allPodsWithLabel := getPodName(oc, ns, "name=test-pods")
			runningPods = getAllPodsWithLabelAndCertainState(oc, ns, "name=test-pods", "Running")
			return len(runningPods) == len(allPodsWithLabel)
		}, "180s", "10s").Should(o.BeTrue(), "Terminating pods did not disappear after waiting enough time")

		exutil.By("6. Scale test pods down to 0 \n")
		scaleErr = oc.AsAdmin().WithoutNamespace().Run("scale").Args("rc", "test-rc", "--replicas=0", "-n", ns).Execute()
		o.Expect(scaleErr).NotTo(o.HaveOccurred())

		o.Eventually(func() bool {
			terminatingPods = getAllPodsWithLabelAndCertainState(oc, ns, "name=test-pods", "Terminating")
			return len(terminatingPods) == 1
		}, "30s", "5s").Should(o.BeTrue(), "Test pods did not scale down to 0")
		e2e.Logf("\n terminatingPods: %v\n", terminatingPods)

		exutil.By("6.1. Check lb-list entries in northdb again in each node's ovnkube-node pod, verify that the two terminating but serving pods reminded in service lb endpoints \n")

		// expectedRemindedEPv4 are still expected in NBDB for a little while,
		// that is because the last pod transition from Running state to terminating but serving state and there is no other running pod available
		for _, eachNode := range nodeList {
			actualFinalEPsv4, epErr = getLBListEndpointsbySVCIPPortinNBDB(oc, eachNode, svcIPv4+":27017")
			o.Expect(epErr).NotTo(o.HaveOccurred())
			e2e.Logf("\n\n After scale-down to 0, V4 endpoints from lb-list output on node %s northdb: %v\n\n", eachNode, actualFinalEPsv4)
			o.Expect(unorderedEqual(actualFinalEPsv4, expectedRemindedEPsv4)).Should(o.BeTrue(), fmt.Sprintf("After scale-down, V4 service lb endpoints on node %s do not match expected endpoints!", eachNode))
		}

		exutil.By("6.2 Verify that the terminating pod still receives traffic because there is no other running pod\n")
		for i, pod := range terminatingPods {
			go func(i int, pod string) {
				defer g.GinkgoRecover()
				tcpdumpCmd := fmt.Sprintf(`timeout 60s tcpdump -c 4 -nneep -i any "(dst port 8080) and (dst %s)"`, podIPv4s[pod])
				outputTcpdump, _ := oc.AsAdmin().WithoutNamespace().Run("debug").Args("-n", "default", "node/"+podNodeNames[pod], "--", "bash", "-c", tcpdumpCmd).Output()
				channels[i] <- outputTcpdump
			}(i, pod)
		}

		// add sleep time to let the ping action happen later after tcpdump is enabled.
		time.Sleep(5 * time.Second)
		output, curlErr = exec.Command("bash", "-c", curlSVC4ChkCmd).Output()
		o.Expect(curlErr).NotTo(o.HaveOccurred())

		for i, pod := range terminatingPods {
			receivedMsg := <-channels[i]
			e2e.Logf(" at step 6.2, tcpdumpOutput for node %s is \n%s\n\n", podNodeNames[pod], receivedMsg)
			o.Expect(strings.Contains(receivedMsg, podIPv4s[pod])).Should(o.BeTrue())
		}
		o.Expect(strings.Contains(string(output), "Hello OpenShift")).Should(o.BeTrue(), "The externalIP service is not reachable as expected")
	})

})
